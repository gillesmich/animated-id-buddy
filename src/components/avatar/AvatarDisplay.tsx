import { useState, useRef, useEffect, useCallback } from "react";
import { Card } from "@/components/ui/card";
import { Button } from "@/components/ui/button";
import { Input } from "@/components/ui/input";
import { Send, Loader2, Video, Play } from "lucide-react";
import { useToast } from "@/components/ui/use-toast";
import VoiceControls from "./VoiceControls";
import ErrorOverlay from "./ErrorOverlay";
import { debounce } from "@/utils/audioUtils";
import { VideoTransitionManager } from "@/utils/videoTransitions";
import { authenticatedFetch } from "@/utils/authenticatedFetch";
import { DIDWebRTCManager } from "@/utils/didWebRTC";
import { getAvatarImage, getAvatarForDID } from "@/config/avatars";
import "./avatar-transitions.css";

interface AvatarDisplayProps {
  config: {
    didApiKey: string;
    openaiApiKey: string;
    elevenlabsApiKey: string;
    selectedAvatar: string;
    customAvatarImage: string;
    customAvatarVideo?: string;
    selectedVoice: string;
    selectedModel: string;
    selectedWorkflow: string;
    workflows: Array<{ id: string; name: string; webhookUrl: string }>;
    useN8n?: boolean;
    avatarProvider?: 'did' | 'musetalk';
  };
}

const AvatarDisplay = ({ config }: AvatarDisplayProps) => {
  const [message, setMessage] = useState("");
  const [isLoading, setIsLoading] = useState(false);
  const [conversation, setConversation] = useState<Array<{ role: string; content: string; type?: 'text' | 'voice' }>>([]);
  const [streamingText, setStreamingText] = useState("");
  const [apiError, setApiError] = useState<{ title: string; message: string; timestamp: Date } | null>(null);
  const [isAvatarSpeaking, setIsAvatarSpeaking] = useState(false);
  
  // Charger les vid√©os depuis localStorage au d√©marrage
  const [generatedVideos, setGeneratedVideos] = useState<Array<{ url: string; text: string; timestamp: Date }>>(() => {
    try {
      const saved = localStorage.getItem('generatedVideos');
      if (saved) {
        const parsed = JSON.parse(saved);
        // Convertir les timestamps string en Date
        return parsed.map((v: any) => ({
          ...v,
          timestamp: new Date(v.timestamp)
        }));
      }
    } catch (error) {
      console.error("Erreur chargement vid√©os:", error);
    }
    return [];
  });
  const { toast } = useToast();
  const conversationEndRef = useRef<HTMLDivElement>(null);
  const videoRef = useRef<HTMLVideoElement>(null);
  const secondaryVideoRef = useRef<HTMLVideoElement>(null);
  const transitionManagerRef = useRef<VideoTransitionManager | null>(null);
  const peerConnectionRef = useRef<RTCPeerConnection | null>(null);
  const streamIdRef = useRef<string | null>(null);
  const sessionIdRef = useRef<string | null>(null);
  const pendingStreamRef = useRef<MediaStream | null>(null);
  const [isStreaming, setIsStreaming] = useState(false);
  const [webRTCStatus, setWebRTCStatus] = useState<string>("");
  const webRTCManagerRef = useRef<any>(null);

  const [avatarForDID, setAvatarForDID] = useState<{ presenterId?: string; url?: string }>(getAvatarForDID('amy'));
  const [currentVideoUrl, setCurrentVideoUrl] = useState<string>(getAvatarImage('amy'));
  const [isVideoLoading, setIsVideoLoading] = useState(false);

  // Auto-scroll to latest message
  useEffect(() => {
    conversationEndRef.current?.scrollIntoView({ behavior: "smooth" });
  }, [conversation, streamingText]);

  // Initialiser le gestionnaire de transitions
  useEffect(() => {
    if (videoRef.current && secondaryVideoRef.current) {
      transitionManagerRef.current = new VideoTransitionManager(
        videoRef.current,
        secondaryVideoRef.current,
        "" // Pas de vid√©o idle par d√©faut
      );

      console.log("üé¨ Gestionnaire de transitions initialis√©");
    }

    return () => {
      transitionManagerRef.current?.cleanup();
    };
  }, [config.customAvatarImage, config.customAvatarVideo, config.selectedAvatar, config.avatarProvider, isStreaming]);

  // Load avatar preview when selection changes
  useEffect(() => {
    console.log("üîÑ Avatar config complet:", { 
      selectedAvatar: config.selectedAvatar, 
      customAvatarImage: config.customAvatarImage,
      customAvatarVideo: config.customAvatarVideo,
      provider: config.avatarProvider,
      hasVideo: !!config.customAvatarVideo,
      videoLength: config.customAvatarVideo?.length
    });
    
    // Pour MuseTalk, priorit√© ABSOLUE √† la vid√©o personnalis√©e
    if (config.avatarProvider === 'musetalk') {
      if (config.customAvatarVideo && config.customAvatarVideo.trim() !== '') {
        console.log("üìπ ‚úÖ Chargement vid√©o personnalis√©e pour MuseTalk:", config.customAvatarVideo);
        setAvatarForDID({ url: config.customAvatarVideo });
        setCurrentVideoUrl(config.customAvatarVideo);
        return;
      } else {
        console.warn("‚ö†Ô∏è MuseTalk activ√© mais AUCUNE vid√©o upload√©e!");
        console.warn("‚ö†Ô∏è Uploadez une vid√©o dans l'onglet Upload pour utiliser MuseTalk");
      }
    }
    
    // Priorit√© √† l'image personnalis√©e (si elle existe vraiment)
    if (config.customAvatarImage && config.customAvatarImage.trim() !== '') {
      console.log("üì∏ Chargement image personnalis√©e");
      setAvatarForDID({ url: config.customAvatarImage });
      setCurrentVideoUrl(config.customAvatarImage);
    } else if (config.selectedAvatar) {
      // Utiliser presenter ID ou URL pour D-ID API
      const didConfig = getAvatarForDID(config.selectedAvatar);
      const localUrl = getAvatarImage(config.selectedAvatar);
      console.log("üì∏ Chargement avatar:", { didConfig, localUrl });
      setAvatarForDID(didConfig);
      setCurrentVideoUrl(localUrl);  // URL locale pour affichage UI
    } else {
      console.log("‚ö†Ô∏è Aucun avatar configur√© - utilisation avatar par d√©faut");
      const defaultDIDConfig = getAvatarForDID('amy');
      const defaultLocalUrl = getAvatarImage('amy');
      console.log("üì∏ Avatar par d√©faut:", { defaultDIDConfig, defaultLocalUrl });
      setAvatarForDID(defaultDIDConfig);
      setCurrentVideoUrl(defaultLocalUrl);
    }
  }, [config.selectedAvatar, config.customAvatarImage, config.customAvatarVideo, config.avatarProvider]);





  // D√©marrer une session WebRTC avec D-ID
  const startWebRTCSession = async () => {
    if (!avatarForDID.presenterId && !avatarForDID.url) {
      toast({
        title: "Avatar manquant",
        description: "S√©lectionnez d'abord un avatar",
        variant: "destructive",
      });
      return;
    }

    if (!videoRef.current) {
      toast({
        title: "Erreur",
        description: "√âl√©ment vid√©o non disponible",
        variant: "destructive",
      });
      return;
    }

    setIsVideoLoading(true);
    setIsStreaming(true);

    try {
      console.log("üé¨ D√©marrage session WebRTC D-ID");
      console.log("üì∏ Avatar config:", avatarForDID);
      
      // Cr√©er le gestionnaire WebRTC
      webRTCManagerRef.current = new DIDWebRTCManager(
        videoRef.current,
        (status) => {
          console.log("üìä Statut WebRTC:", status);
          setWebRTCStatus(status);
        }
      );

      // Cr√©er la session (use URL for now, WebRTC might need URL)
      const imageUrl = avatarForDID.url || '';
      await webRTCManagerRef.current.createSession(imageUrl);
      
      toast({
        title: "‚úÖ Connexion √©tablie",
        description: "L'avatar est pr√™t en mode WebRTC",
      });

      // Envoyer un message de test
      await webRTCManagerRef.current.sendText(
        "Bonjour! Je suis votre assistant virtuel en streaming WebRTC. Comment puis-je vous aider?",
        'fr-FR-DeniseNeural'
      );

      setIsVideoLoading(false);
    } catch (error) {
      console.error("‚ùå Erreur WebRTC:", error);
      setIsVideoLoading(false);
      setIsStreaming(false);
      
      setApiError({
        title: "Erreur WebRTC",
        message: error instanceof Error ? error.message : "Impossible de d√©marrer la session WebRTC",
        timestamp: new Date()
      });
      
      toast({
        title: "Erreur",
        description: "Impossible de d√©marrer le streaming WebRTC",
        variant: "destructive",
      });
    }
  };

  // Arr√™ter la session WebRTC
  const stopWebRTCSession = () => {
    if (webRTCManagerRef.current) {
      webRTCManagerRef.current.cleanup();
      webRTCManagerRef.current = null;
    }
    setIsStreaming(false);
    setWebRTCStatus("");
  };

  // Nettoyer la session WebRTC au d√©montage
  useEffect(() => {
    return () => {
      stopWebRTCSession();
    };
  }, []);

  const sendToWorkflow = async (messageText: string, audioBase64?: string) => {
    const selectedWorkflow = config.workflows.find(w => w.id === config.selectedWorkflow);
    
    if (!selectedWorkflow) {
      throw new Error("Aucun workflow s√©lectionn√©");
    }

    try {
      const response = await fetch(selectedWorkflow.webhookUrl, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({
          message: messageText,
          avatarId: config.customAvatarImage || config.selectedAvatar,
          voiceId: config.selectedVoice,
          model: config.selectedModel,
          audio: audioBase64,
          timestamp: new Date().toISOString()
        }),
      });

      if (!response.ok) {
        throw new Error(`Erreur workflow: ${response.statusText}`);
      }

      return await response.json();
    } catch (error) {
      console.error('Workflow error:', error);
      throw error;
    }
  };

  const handleSendMessage = async () => {
    if (!message.trim()) return;

    // Mode WebRTC - envoyer le message directement au stream
    if (isStreaming && webRTCManagerRef.current) {
      const userMessage = message.trim();
      setMessage("");
      
      setConversation(prev => [
        ...prev,
        { role: "user", content: userMessage, type: 'text' }
      ]);

      setIsLoading(true);
      try {
        // En mode WebRTC, on envoie simplement le texte pour animation
        await webRTCManagerRef.current.sendText(userMessage, 'fr-FR-DeniseNeural');
        
        setConversation(prev => [
          ...prev,
          { role: "assistant", content: userMessage, type: 'text' }
        ]);
      } catch (error) {
        console.error("Error sending WebRTC message:", error);
        toast({
          title: "Erreur",
          description: "Impossible d'envoyer le message",
          variant: "destructive",
        });
      } finally {
        setIsLoading(false);
      }
      return;
    }

    // Validation pour mode REST classique
    if (!config.didApiKey || !config.openaiApiKey || !config.elevenlabsApiKey) {
      toast({
        title: "Configuration manquante",
        description: "Veuillez configurer toutes les cl√©s API",
        variant: "destructive",
      });
      return;
    }

    if (!config.selectedAvatar || !config.selectedVoice || !config.selectedModel) {
      toast({
        title: "S√©lection manquante",
        description: "Veuillez s√©lectionner avatar, voix et mod√®le",
        variant: "destructive",
      });
      return;
    }

    setIsLoading(true);
    setConversation([...conversation, { role: "user", content: message, type: 'text' }]);
    const userMessage = message;
    setMessage("");

    try {
      // Mode n8n workflow
      if (config.useN8n && config.selectedWorkflow) {
        console.log("üîÄ Utilisation du workflow n8n");
        const result = await sendToWorkflow(userMessage);
        
        // Simulate streaming text
        const responseText = result.text || "R√©ponse du workflow re√ßue avec succ√®s";
        let currentText = "";
        
        for (let i = 0; i < responseText.length; i++) {
          currentText += responseText[i];
          setStreamingText(currentText);
          await new Promise(resolve => setTimeout(resolve, 30));
        }
        
        setConversation((prev) => [
          ...prev,
          { role: "assistant", content: responseText, type: 'text' },
        ]);
        setStreamingText("");
        
        toast({
          title: "R√©ponse re√ßue",
          description: "Le workflow a trait√© votre message",
        });
      } 
      // Mode Python Backend (par d√©faut)
      else {
        console.error("Backend Python non configur√©");
      }
    } catch (error) {
      console.error('Send message error:', error);
      
      const errorMessage = error instanceof Error ? error.message : "√âchec de l'envoi";
      
      setApiError({
        title: "Erreur d'envoi",
        message: errorMessage,
        timestamp: new Date()
      });
      
      toast({
        title: "Erreur",
        description: errorMessage,
        variant: "destructive",
      });
    } finally {
      setIsLoading(false);
    }
  };

  const handleVoiceMessage = async (audioBase64: string) => {
    setIsLoading(true);

    try {
      // √âtape 1: Transcription avec Whisper
      console.log("üé§ √âtape 1: Transcription Whisper...");
      toast({
        title: "üé§ Transcription...",
        description: "Analyse de votre message vocal",
      });

      const transcriptionResponse = await authenticatedFetch('whisper-transcribe', {
        method: 'POST',
        body: JSON.stringify({ audioBase64 }),
      });

      if (!transcriptionResponse.ok) {
        throw new Error('Erreur de transcription');
      }

      const { text: transcription } = await transcriptionResponse.json();
      console.log("‚úÖ Transcription:", transcription);
      
      // Filtrage: ignorer les transcriptions vides ou trop courtes
      const cleanTranscription = transcription.trim();
      if (!cleanTranscription || cleanTranscription.length < 5) {
        console.log("‚ö†Ô∏è Transcription trop courte ou vide, ignor√©e:", cleanTranscription);
        setIsLoading(false);
        return;
      }
      
      // Filtrage: ignorer les phrases de remerciement g√©n√©riques
      const genericPhrases = ["merci √† tous", "au revoir", "merci et"];
      if (genericPhrases.some(phrase => cleanTranscription.toLowerCase().includes(phrase)) && cleanTranscription.length < 30) {
        console.log("‚ö†Ô∏è Phrase g√©n√©rique d√©tect√©e, ignor√©e:", cleanTranscription);
        setIsLoading(false);
        return;
      }
      
      setConversation((prev) => [...prev, { role: "user", content: cleanTranscription, type: 'voice' }]);

      // √âtape 2: G√©n√©ration de r√©ponse avec OpenAI
      console.log("ü§ñ √âtape 2: G√©n√©ration r√©ponse OpenAI...");
      toast({
        title: "ü§ñ R√©flexion...",
        description: "G√©n√©ration de la r√©ponse",
      });

      const chatResponse = await authenticatedFetch('openai-chat', {
        method: 'POST',
        body: JSON.stringify({
          messages: [
            { role: 'system', content: 'Tu es un assistant virtuel intelligent et amical. R√©ponds de mani√®re concise et naturelle.' },
            { role: 'user', content: cleanTranscription }
          ],
          model: config.selectedModel || 'gpt-4o-mini',
        }),
      });

      if (!chatResponse.ok) {
        throw new Error('Erreur de g√©n√©ration de r√©ponse');
      }

      const chatData = await chatResponse.json();
      const responseText = chatData.choices[0].message.content;
      console.log("‚úÖ R√©ponse OpenAI:", responseText);

      // Affichage streaming de la r√©ponse
      let currentText = "";
      for (let i = 0; i < responseText.length; i++) {
        currentText += responseText[i];
        setStreamingText(currentText);
        await new Promise(resolve => setTimeout(resolve, 20));
      }
      
      setConversation((prev) => [
        ...prev,
        { role: "assistant", content: responseText, type: 'text' },
      ]);
      setStreamingText("");

      // √âtape 3: G√©n√©ration vid√©o avec provider s√©lectionn√©
      const provider = config.avatarProvider || 'did';
      console.log(`üé¨ √âtape 3: G√©n√©ration vid√©o ${provider.toUpperCase()}...`);
      console.log('üìã Config compl√®te:', { 
        avatarProvider: config.avatarProvider,
        provider,
        avatarForDID,
        currentVideoUrl 
      });
      
      if (!avatarForDID.presenterId && !avatarForDID.url) {
        console.log("‚ö†Ô∏è Pas d'avatar configur√©");
        toast({
          title: "Avatar manquant",
          description: "Veuillez s√©lectionner un avatar dans la configuration",
          variant: "destructive",
        });
        return;
      }
      
      console.log("üì∏ Avatar config:", avatarForDID);
      
      // Validation de la longueur du texte
      let textForVideo = responseText;
      if (textForVideo.length > 1000) {
        console.warn("‚ö†Ô∏è Texte trop long, troncature √† 1000 caract√®res");
        textForVideo = textForVideo.substring(0, 997) + "...";
      }
      
      setIsVideoLoading(true);
      toast({
        title: "üé¨ G√©n√©ration vid√©o...",
        description: "Cr√©ation de l'animation",
      });
      
      try {
        let videoUrl: string;

        if (provider === 'musetalk') {
          // PRIORIT√â √Ä config.customAvatarVideo pour √©viter les probl√®mes de timing
          console.log("üéØ DEBUG MuseTalk:");
          console.log("  - config.customAvatarVideo:", config.customAvatarVideo);
          console.log("  - avatarForDID.url:", avatarForDID.url);
          console.log("  - currentVideoUrl:", currentVideoUrl);
          
          const sourceUrl = config.customAvatarVideo || avatarForDID.url || currentVideoUrl;
          
          if (!sourceUrl || sourceUrl.match(/\.(jpg|jpeg|png|gif)$/i)) {
            console.error("‚ùå MuseTalk: Pas de vid√©o ou source est une image");
            toast({
              title: "üìπ Vid√©o requise",
              description: "Uploadez une vid√©o dans Configuration ‚Üí Onglet 'Upload vid√©o' pour utiliser MuseTalk",
              variant: "destructive",
              duration: 8000,
            });
            setIsVideoLoading(false);
            return;
          }
          
          console.log("‚úÖ MuseTalk - Source vid√©o valid√©e:", sourceUrl);

          // Upload local video to Supabase Storage to get a publicly accessible URL
          const { uploadLocalImageToStorage } = await import('@/utils/uploadImageToStorage');
          let videoUrl: string;
          
          if (sourceUrl.startsWith('http')) {
            videoUrl = sourceUrl;
          } else {
            videoUrl = await uploadLocalImageToStorage(
              `${window.location.origin}${sourceUrl}`
            );
          }

          console.log("üìπ Public video URL:", videoUrl);

          // Appel √† FAL AI MuseTalk via edge function
          const requestBody = {
            action: 'create_talk',
            data: {
              source_url: videoUrl,
              text: textForVideo,
              voice_id: config.selectedVoice,
              config: {
                bbox_shift: 0
              }
            }
          };

          const talkResponse = await authenticatedFetch('musetalk-avatar', {
            method: 'POST',
            body: JSON.stringify(requestBody),
          });

          if (!talkResponse.ok) {
            const errorData = await talkResponse.json().catch(() => ({}));
            console.error('‚ùå Erreur FAL MuseTalk:', talkResponse.status, errorData);
            
            if (errorData.code === 'INVALID_SOURCE_TYPE') {
              toast({
                title: "üìπ Vid√©o requise",
                description: errorData.message,
                variant: "destructive",
                duration: 8000,
              });
              throw new Error(errorData.message);
            }
            
            if (errorData.code === 'NOT_CONFIGURED') {
              throw new Error("FAL API key non configur√©e");
            }
            throw new Error(`Erreur FAL MuseTalk: ${errorData.error || talkResponse.status}`);
          }

          const talkData = await talkResponse.json();
          videoUrl = talkData.result_url;  // Direct result from FAL AI - no polling needed!
          console.log("‚úÖ FAL MuseTalk vid√©o g√©n√©r√©e:", videoUrl);
        } else {
          // Appel √† D-ID (code existant)
          const requestBody: any = {
            action: 'create_talk',
            data: {
              script: {
                type: 'text',
                input: textForVideo,
                provider: {
                  type: 'microsoft',
                  voice_id: 'fr-FR-DeniseNeural'
                }
              },
              config: {
                fluent: true,
                pad_audio: 0,
                stitch: true,
                result_format: 'mp4'
              }
            }
          };

          if (avatarForDID.presenterId) {
            requestBody.data.presenter_id = avatarForDID.presenterId;
            console.log("üì∏ Utilisation presenter ID:", avatarForDID.presenterId);
          } else if (avatarForDID.url) {
            requestBody.data.source_url = avatarForDID.url;
            console.log("üì∏ Utilisation URL:", avatarForDID.url);
          }

          const talkResponse = await authenticatedFetch('did-avatar', {
            method: 'POST',
            body: JSON.stringify(requestBody),
          });

          if (!talkResponse.ok) {
            const errorData = await talkResponse.json().catch(() => ({}));
            console.error('‚ùå Erreur D-ID:', talkResponse.status, errorData);
            throw new Error(`Erreur D-ID: ${errorData.error || talkResponse.status}`);
          }

          const talkData = await talkResponse.json();
          const talkId = talkData.id;
          console.log("‚úÖ Talk cr√©√©:", talkId);

          // Polling pour attendre la vid√©o D-ID
          let attempts = 0;
          const maxAttempts = 60;
          
          const pollVideo = async (): Promise<string> => {
            attempts++;
            
            if (attempts > maxAttempts) {
              throw new Error("Timeout g√©n√©ration vid√©o");
            }

            const statusResponse = await authenticatedFetch('did-avatar', {
              method: 'POST',
              body: JSON.stringify({
                action: 'get_talk',
                data: { talkId }
              }),
            });

            if (!statusResponse.ok) {
              throw new Error("Erreur v√©rification statut");
            }

            const statusData = await statusResponse.json();
            console.log(`üìä Statut (${attempts}/${maxAttempts}):`, statusData.status);

            if (statusData.status === 'done' && statusData.result_url) {
              return statusData.result_url;
            } else if (statusData.status === 'error') {
              throw new Error(`Erreur D-ID: ${statusData.error?.description || 'Inconnue'}`);
            }

            await new Promise(resolve => setTimeout(resolve, 2000));
            return pollVideo();
          };

          videoUrl = await pollVideo();
          console.log("‚úÖ Vid√©o g√©n√©r√©e:", videoUrl);
        }

        // Jouer la vid√©o avec transition
        if (transitionManagerRef.current) {
          transitionManagerRef.current.transitionToVideo(videoUrl);
          setIsAvatarSpeaking(true);
        }

        // Sauvegarder la vid√©o g√©n√©r√©e
        const newVideo = {
          url: videoUrl,
          text: responseText,
          timestamp: new Date()
        };
        
        setGeneratedVideos(prev => {
          const updated = [...prev, newVideo];
          // Persister dans localStorage
          try {
            localStorage.setItem('generatedVideos', JSON.stringify(updated));
          } catch (error) {
            console.error("Erreur sauvegarde vid√©os:", error);
          }
          return updated;
        });

        setIsVideoLoading(false);
        toast({
          title: "‚úÖ Vid√©o pr√™te",
          description: "L'avatar r√©pond",
        });
      } catch (videoError) {
        console.error("‚ùå Erreur g√©n√©ration vid√©o:", videoError);
        setIsVideoLoading(false);
        // Continuer sans vid√©o - le texte est d√©j√† affich√©
        toast({
          title: "‚ö†Ô∏è Vid√©o non disponible",
          description: "R√©ponse affich√©e en texte",
        });
      }



    } catch (error) {
      console.error('Voice message error:', error);
      
      const errorMessage = error instanceof Error ? error.message : "√âchec du traitement vocal";
      
      setApiError({
        title: "Erreur vocale",
        message: errorMessage,
        timestamp: new Date()
      });
      
      toast({
        title: "Erreur",
        description: errorMessage,
        variant: "destructive",
      });
      
      setIsVideoLoading(false);
    } finally {
      setIsLoading(false);
    }
  };

  // Debounced typing indicator
  const handleTyping = debounce(() => {
    // Typing indicator
  }, 500);

  // G√©rer quand l'utilisateur commence √† parler - transition vers idle
  const handleUserSpeaking = useCallback((speaking: boolean) => {
    console.log(speaking ? "üé§ Utilisateur commence √† parler - passage en idle" : "üé§ Utilisateur a fini de parler");
    
    if (speaking) {
      // Passer en mode idle (image statique) quand l'utilisateur parle
      if (videoRef.current) {
        videoRef.current.pause();
        console.log("‚è∏Ô∏è Avatar en position d'attente");
      }
      setIsAvatarSpeaking(false);
    } else {
      // Ne rien faire ici - l'avatar reprendra quand il aura une nouvelle r√©ponse
      console.log("‚úÖ Utilisateur a fini - en attente de r√©ponse");
    }
  }, []);

  // D√©tecter quand l'avatar parle et g√©rer les transitions
  useEffect(() => {
    const video = videoRef.current;
    if (!video) return;

    const handlePlay = () => {
      setIsAvatarSpeaking(true);
      console.log("üó£Ô∏è Avatar commence √† parler");
    };

    const handlePause = () => {
      setIsAvatarSpeaking(false);
      console.log("ü§´ Avatar arr√™te de parler");
    };

    const handleEnded = async () => {
      setIsAvatarSpeaking(false);
      console.log("‚úÖ Avatar a fini de parler - retour √† l'idle");
      
      // Retourner √† l'idle apr√®s un court d√©lai
      setTimeout(async () => {
        if (transitionManagerRef.current && !isStreaming) {
          await transitionManagerRef.current.returnToIdle();
        }
      }, 500);
    };

    video.addEventListener('play', handlePlay);
    video.addEventListener('pause', handlePause);
    video.addEventListener('ended', handleEnded);

    return () => {
      video.removeEventListener('play', handlePlay);
      video.removeEventListener('pause', handlePause);
      video.removeEventListener('ended', handleEnded);
    };
  }, [isStreaming]);

  return (
    <Card className="glass p-6 space-y-6 h-full">
      <div className="space-y-2">
        <h3 className="text-2xl font-bold flex items-center gap-2">
          <Video className="w-5 h-5 text-primary" />
          Avatar Preview
        </h3>
        <p className="text-sm text-muted-foreground">
          Test your interactive avatar
        </p>
      </div>

      {/* WebRTC Controls - Only shown for DID provider */}
      {config.avatarProvider === 'did' && (
        <div className="flex gap-2 items-center">
          {!isStreaming ? (
            <Button
              onClick={startWebRTCSession}
              disabled={isVideoLoading || (!avatarForDID.presenterId && !avatarForDID.url)}
              className="gradient-primary"
            >
              {isVideoLoading ? (
                <>
                  <Loader2 className="w-4 h-4 animate-spin mr-2" />
                  Connexion WebRTC...
                </>
              ) : (
                <>
                  <Play className="w-4 h-4 mr-2" />
                  D√©marrer WebRTC
                </>
              )}
            </Button>
          ) : (
            <Button
              onClick={stopWebRTCSession}
              variant="destructive"
            >
              Arr√™ter WebRTC
            </Button>
          )}
          {webRTCStatus && (
            <span className="text-sm text-muted-foreground">
              Statut: {webRTCStatus}
            </span>
          )}
        </div>
      )}

      {/* Generated Videos Gallery - Remplace la pr√©visualisation */}
      {generatedVideos.length > 0 ? (
        <div className="space-y-3">
          <div className="flex items-center justify-between">
            <h4 className="text-sm font-semibold flex items-center gap-2">
              <Video className="w-4 h-4 text-primary" />
              Vid√©os de R√©ponse ({generatedVideos.length})
            </h4>
            <Button
              size="sm"
              variant="outline"
              onClick={() => {
                setGeneratedVideos([]);
                localStorage.removeItem('generatedVideos');
                toast({
                  title: "Galerie vid√©e",
                  description: "Toutes les vid√©os ont √©t√© supprim√©es",
                });
              }}
            >
              Vider la galerie
            </Button>
          </div>
          <div className="grid grid-cols-2 gap-3 max-h-96 overflow-y-auto">
            {generatedVideos.map((video, idx) => (
              <div key={idx} className="group relative rounded-lg overflow-hidden border border-border/50 bg-secondary/20">
                <video
                  src={video.url}
                  className="w-full aspect-video object-cover"
                  controls
                  preload="metadata"
                />
                <div className="absolute top-2 right-2 opacity-0 group-hover:opacity-100 transition-opacity">
                  <Button
                    size="sm"
                    variant="secondary"
                    className="h-7 px-2 text-xs"
                    onClick={() => {
                      const a = document.createElement('a');
                      a.href = video.url;
                      a.download = `avatar-response-${idx + 1}.mp4`;
                      a.click();
                      toast({
                        title: "üì• T√©l√©chargement",
                        description: "La vid√©o va √™tre t√©l√©charg√©e",
                      });
                    }}
                  >
                    T√©l√©charger
                  </Button>
                </div>
                <div className="p-2 bg-background/80 backdrop-blur-sm">
                  <p className="text-xs text-muted-foreground line-clamp-2">
                    {video.text}
                  </p>
                  <p className="text-xs text-muted-foreground mt-1">
                    {video.timestamp.toLocaleTimeString()}
                  </p>
                </div>
              </div>
            ))}
          </div>
          
          {/* Voice Controls */}
          <VoiceControls
            onVoiceMessage={handleVoiceMessage}
            isProcessing={isLoading}
            className="justify-center mt-3"
            onUserSpeaking={handleUserSpeaking}
            isAvatarSpeaking={isAvatarSpeaking}
          />
        </div>
      ) : (
        /* Avatar Video Area - Affich√© seulement si aucune vid√©o g√©n√©r√©e */
        <div className="rounded-lg bg-secondary/30 border border-border/50 relative overflow-hidden group">
          <div className="absolute inset-0 gradient-glow opacity-30"></div>
          
          {/* Deux √©l√©ments vid√©o pour les transitions fluides */}
          <div className="relative w-full aspect-video">
            {/* Vid√©o principale */}
            <video
              ref={videoRef}
              className={`absolute inset-0 w-full h-full object-cover avatar-video-transition ${
                isAvatarSpeaking ? 'avatar-speaking' : 'avatar-idle'
              }`}
              autoPlay
              playsInline
              muted={false}
              loop
              src={currentVideoUrl?.match(/\.(mp4|webm|mov)$/i) ? currentVideoUrl : undefined}
              poster={currentVideoUrl?.match(/\.(jpg|jpeg|png|gif)$/i) ? currentVideoUrl : undefined}
              style={{ opacity: 1 }}
            />
            
            {/* Vid√©o secondaire pour transitions */}
            <video
              ref={secondaryVideoRef}
              className="absolute inset-0 w-full h-full object-cover avatar-video-transition"
              playsInline
              muted={false}
              style={{ opacity: 0, display: 'none' }}
            />
            
            {/* Indicateur d'√©tat */}
            {isAvatarSpeaking && (
              <div className="absolute top-4 right-4 px-3 py-2 bg-primary/90 text-primary-foreground text-sm rounded-full flex items-center gap-2 animate-pulse">
                <div className="w-2 h-2 rounded-full bg-white animate-pulse" />
                Parle...
              </div>
            )}
            
            {isStreaming && !isAvatarSpeaking && (
              <div className="absolute top-4 right-4 px-3 py-2 bg-green-500/90 text-white text-sm rounded-full flex items-center gap-2">
                <div className="w-2 h-2 rounded-full bg-white" />
                Pr√™t
              </div>
            )}
            
            {!isStreaming && !isAvatarSpeaking && (
              <div className="absolute top-4 right-4 px-3 py-2 bg-yellow-500/90 text-white text-sm rounded-full animate-pulse">
                Connexion...
              </div>
            )}
          </div>
              
          
          {/* Voice Controls - Positionn√©s directement sous la vid√©o */}
          <VoiceControls
            onVoiceMessage={handleVoiceMessage}
            isProcessing={isLoading}
            className="justify-center mt-3"
            onUserSpeaking={handleUserSpeaking}
            isAvatarSpeaking={isAvatarSpeaking}
          />
        </div>
      )}

      {/* Chat Interface */}
      <div className="space-y-4">
        <div className="h-64 overflow-y-auto space-y-3 p-4 rounded-lg bg-secondary/20 border border-border/30">
          {conversation.length === 0 ? (
            <div className="h-full flex items-center justify-center text-muted-foreground text-sm">
              D√©marrez une conversation avec votre avatar
            </div>
          ) : (
            <>
              {conversation.map((msg, idx) => (
                <div
                  key={idx}
                  className={`p-3 rounded-lg animate-fade-in ${
                    msg.role === "user"
                      ? "bg-primary/20 ml-auto max-w-[80%]"
                      : "bg-secondary/50 mr-auto max-w-[80%]"
                  }`}
                >
                  <p className="text-sm">{msg.content}</p>
                  {msg.type === 'voice' && (
                    <span className="text-xs text-muted-foreground">üé§</span>
                  )}
                </div>
              ))}
              {streamingText && (
                <div className="p-3 rounded-lg bg-secondary/50 mr-auto max-w-[80%] animate-pulse">
                  <p className="text-sm">{streamingText}</p>
                </div>
              )}
            </>
          )}
          {isLoading && !streamingText && (
            <div className="flex items-center gap-2 text-muted-foreground p-3">
              <Loader2 className="w-4 h-4 animate-spin" />
              <span className="text-sm">Avatar r√©fl√©chit...</span>
            </div>
          )}
          <div ref={conversationEndRef} />
        </div>

        {/* Text Input */}
        <div className="flex gap-2">
          <Input
            placeholder="Tapez votre message..."
            value={message}
            onChange={(e) => {
              setMessage(e.target.value);
              handleTyping();
            }}
            onKeyPress={(e) => {
              if (e.key === "Enter" && !isLoading) {
                handleSendMessage();
              }
            }}
            className="glass"
            disabled={isLoading}
            autoComplete="off"
          />
          <Button
            onClick={handleSendMessage}
            disabled={isLoading || !message.trim()}
            className="gradient-primary"
          >
            {isLoading ? (
              <Loader2 className="w-4 h-4 animate-spin" />
            ) : (
              <Send className="w-4 h-4" />
            )}
          </Button>
        </div>
      </div>


      {/* Error Overlay */}
      <ErrorOverlay 
        error={apiError}
        onClose={() => setApiError(null)}
      />
    </Card>
  );
};

export default AvatarDisplay;
